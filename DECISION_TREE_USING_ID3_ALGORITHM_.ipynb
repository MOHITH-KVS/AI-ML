{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr5HrbfXQqAF",
        "outputId": "5af3917d-bbff-4cfb-d929-a9994cff33bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree:\n",
            "Outlook\n",
            "|   overcast -> yes\n",
            "|   rain -> |   |   Wind\n",
            "|   |   |   strong -> no\n",
            "|   |   |   weak -> yes\n",
            "|   sunny -> |   |   Humidity\n",
            "|   |   |   high -> no\n",
            "|   |   |   normal -> yes\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For handling the dataset\n",
        "import numpy as np  # For numerical operations\n",
        "from math import log2  # For calculating logarithms\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/id3.csv'  # Path to the uploaded dataset\n",
        "data = pd.read_csv(file_path)  # Read the CSV file into a pandas DataFrame\n",
        "\n",
        "# Function to calculate entropy\n",
        "def entropy(data, target_attr):\n",
        "    values, counts = np.unique(data[target_attr], return_counts=True)  # Find unique classes and their counts\n",
        "    entropy_value = 0  # Start with 0 entropy\n",
        "    for i in range(len(values)):  # Loop through each class\n",
        "        proportion = counts[i] / sum(counts)  # Fraction of rows for this class\n",
        "        entropy_value += -proportion * log2(proportion)  # Add entropy for this class\n",
        "    return entropy_value\n",
        "\n",
        "\n",
        "# Function to calculate information gain\n",
        "def information_gain(data, feature, target_attr):\n",
        "    # Calculate the entropy of the entire dataset\n",
        "    total_entropy = entropy(data, target_attr)\n",
        "    # Get the unique values and their counts for the feature\n",
        "    values, counts = np.unique(data[feature], return_counts=True)\n",
        "    # Calculate the weighted entropy for each value of the feature\n",
        "    weighted_entropy = sum(\n",
        "        (counts[i] / sum(counts)) * entropy(data[data[feature] == values[i]], target_attr)\n",
        "        for i in range(len(values))\n",
        "    )\n",
        "    # Information gain is the reduction in entropy\n",
        "    gain = total_entropy - weighted_entropy\n",
        "    return gain\n",
        "\n",
        "# Function to build the decision tree using the ID3 algorithm\n",
        "def id3(data, features, target_attr, parent_node_class=None):\n",
        "\n",
        "    # Base case 1: If all target values are the same, return that class\n",
        "    if len(np.unique(data[target_attr])) == 1:\n",
        "        return np.unique(data[target_attr])[0]\n",
        "\n",
        "    # Base case 2: If dataset is empty or no features left, return the majority class\n",
        "    elif len(data) == 0 or len(features) == 0:\n",
        "        return parent_node_class\n",
        "\n",
        "    # Determine the majority class of the current data\n",
        "    parent_node_class = data[target_attr].mode()[0]\n",
        "\n",
        "    # Choose the best feature to split on using information gain\n",
        "    gains = {feature: information_gain(data, feature, target_attr) for feature in features}\n",
        "    best_feature = max(gains, key=gains.get)\n",
        "\n",
        "    # Create the tree structure with the best feature as the root\n",
        "    tree = {best_feature: {}}\n",
        "    # Remove the best feature from the list of features\n",
        "    features = [f for f in features if f != best_feature]\n",
        "\n",
        "    # Split the dataset and build subtrees recursively for each value of the best feature\n",
        "    for value in np.unique(data[best_feature]):\n",
        "        # Subset of the data for the current value of the feature\n",
        "        subset = data[data[best_feature] == value]\n",
        "        # Recursively build the subtree\n",
        "        subtree = id3(subset, features, target_attr, parent_node_class)\n",
        "        # Add the subtree to the tree\n",
        "        tree[best_feature][value] = subtree\n",
        "\n",
        "    return tree\n",
        "\n",
        "# Define the features and the target attribute\n",
        "features = ['Outlook', 'Temperature', 'Humidity', 'Wind']  # Predictors\n",
        "target_attr = 'PlayTennis'  # Target variable\n",
        "\n",
        "# Build the decision tree\n",
        "decision_tree = id3(data, features, target_attr)\n",
        "# Function to print the decision tree in a tree-like structure\n",
        "def print_tree(tree, depth=0):\n",
        "    if isinstance(tree, dict):\n",
        "        for key, value in tree.items():\n",
        "            print(f\"{'|   ' * depth}{key}\")\n",
        "            if isinstance(value, dict):\n",
        "                for sub_key, sub_value in value.items():\n",
        "                    print(f\"{'|   ' * (depth + 1)}{sub_key} ->\", end=\" \")\n",
        "                    print_tree(sub_value, depth + 2)\n",
        "            else:\n",
        "                print_tree(value, depth + 1)\n",
        "    else:\n",
        "        print(f\"{tree}\")\n",
        "\n",
        "\n",
        "# Print the resulting decision tree in a readable format\n",
        "print(\"Decision Tree:\")\n",
        "print_tree(decision_tree)\n",
        "\n"
      ]
    }
  ]
}